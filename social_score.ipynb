{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add nodes for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network_analysis import sorted_node_lifespan, find_interaction, extract_conversation, node_communication_frequency\n",
    "from conti_network import find_users\n",
    "from clean import time_parser\n",
    "import datetime \n",
    "import networkx as nx\n",
    "G = nx.Graph() \n",
    "# remove admin nodes\n",
    "# remove admins, test accounts, users with lifespan <30days\n",
    "# def clean_users():\n",
    "#     users = sorted_node_lifespan()\n",
    "#     return_users = []\n",
    "#     for u in users.keys():\n",
    "#         if users[u] >= datetime.timedelta(days = 30):\n",
    "#             if 'admin' not in u and 'alarm' not in u:\n",
    "#                 return_users.append(u)\n",
    "#     return return_users\n",
    "Users = find_users('user_lists/users.txt')\n",
    "G.add_nodes_from(Users)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_chat_timestamps():\n",
    "    chat_logs = time_parser('logs/chat_logs.json')\n",
    "    jabber_logs = time_parser('logs/jabber_logs.json')\n",
    "    messages = {}\n",
    "    with open('user_lists/users.txt') as f:\n",
    "        users = f.read().splitlines()\n",
    "    for i in users:\n",
    "        messages[i] = {}\n",
    "    # for i in users:\n",
    "    #     for j in users:\n",
    "    #         messages[i][j] = []\n",
    "    for i in chat_logs:\n",
    "        sender = i['from']\n",
    "        receiver = i['to']\n",
    "        if receiver in messages[sender].keys():\n",
    "            messages[sender][receiver].append(i['ts'])\n",
    "        else: \n",
    "            messages[sender][receiver] = [i['ts']]\n",
    "    for i in jabber_logs:\n",
    "        sender = i['from']\n",
    "        receiver = i['to']\n",
    "        if receiver in messages[sender].keys():\n",
    "            messages[sender][receiver].append(i['ts'])\n",
    "        else: \n",
    "            messages[sender][receiver] = [i['ts']]\n",
    "    for i in messages.keys():\n",
    "        for j in messages[i].keys():\n",
    "            messages[i][j] = sorted(messages[i][j])\n",
    "    return messages\n",
    "interactions = all_chat_timestamps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add edges based on meaningful conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edges(interactions):\n",
    "    for user1 in interactions.keys():\n",
    "        for user2 in interactions[user1].keys():\n",
    "            inter1 = interactions[user1][user2]\n",
    "            if user2 in interactions.keys():\n",
    "                if user1 in interactions[user2].keys():\n",
    "                    inter2 = interactions[user2][user1]\n",
    "                else:\n",
    "                    inter2 = []\n",
    "            else: inter2 = []\n",
    "            both = {user1:inter1, user2:inter2}\n",
    "            wgt = len(extract_conversation(both))\n",
    "            #ADD SOMETHING FOR IF FROM ONE USER TO SAME USER\n",
    "            if wgt > 0:\n",
    "                G.add_edge(user1, user2, key='edge', weight = wgt)\n",
    "generate_edges(interactions)\n",
    "#print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dictionary):\n",
    "    n = min(dictionary.values())\n",
    "    m = max(dictionary.values())\n",
    "    for i in dictionary.keys():\n",
    "        dictionary[i] = (dictionary[i]-n) / (m-n)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/networkx/algorithms/link_analysis/hits_alg.py:78: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A = nx.adjacency_matrix(G, nodelist=list(G), dtype=float)\n"
     ]
    }
   ],
   "source": [
    "user_cliques = {}\n",
    "degrees = G.degree()\n",
    "#degrees_dict = {degrees[i][0]: degrees[i][1] for i in range(0, len(degrees))}\n",
    "degrees_dict = {}\n",
    "for i in degrees:\n",
    "    degrees_dict[i[0]] = i[1]\n",
    "degrees_dict = normalize(degrees_dict)\n",
    "betweeness_centrality_dict = normalize(nx.betweenness_centrality(G))\n",
    "degree_centrality_dict = normalize(nx.degree_centrality(G))\n",
    "hubs_authorities = normalize(nx.hits(G)[0])\n",
    "user_clustering_coefficient = {}\n",
    "user_shortest_path = {}\n",
    "for node in Users:\n",
    "    user_cliques[node] = len(nx.cliques_containing_node(G, node))\n",
    "    cur_clustering_coefficient = nx.clustering(G, nodes = node)\n",
    "    user_clustering_coefficient[node] = cur_clustering_coefficient\n",
    "    shortest_path = nx.shortest_path_length(G, source=node).values()\n",
    "    shortest_path_val = sum(shortest_path) / (len(shortest_path))\n",
    "    user_shortest_path[node] = shortest_path_val\n",
    "user_clustering_coefficient = normalize(user_clustering_coefficient)\n",
    "user_shortest_path = normalize(user_shortest_path)\n",
    "user_cliques = normalize(user_cliques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate social score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_score = {}\n",
    "for u in Users:\n",
    "    social_score[u] = ((degrees_dict[u] + betweeness_centrality_dict[u] + degree_centrality_dict[u] + hubs_authorities[u] + user_clustering_coefficient[u] + user_shortest_path[u] + user_cliques[u])/7)*100\n",
    "social_score = dict(sorted(social_score.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Pandas dataframe to store our values in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0    Degree\n",
      "0x00lord   0.000000  0.000000\n",
      "ahtung     0.000000  0.000000\n",
      "air        0.000000  0.000000\n",
      "airbnb1    0.000000  0.000000\n",
      "alaska     0.000000  0.000000\n",
      "...             ...       ...\n",
      "buza      43.509731  0.600000\n",
      "bentley   45.270605  0.550000\n",
      "mango     47.821786  0.635714\n",
      "defender  59.936479  0.764286\n",
      "stern     76.684920  1.000000\n",
      "\n",
      "[449 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(social_score, orient='index', column)\n",
    "df[\"Degree\"] = pd.Series(degrees_dict)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
