{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports necessary functions from our functions library and other useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import find_users, time_parser\n",
    "import networkx as nx\n",
    "import datetime\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializes our NetworkX Graph and adds a node for each user. \n",
    "Users are found by pulling usernames from the text file users.txt. This file contains all recorded users from the chat logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph() \n",
    "Users = find_users('user_lists/users.txt')\n",
    "G.add_nodes_from(Users)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we find interactions. This function will create a JSON object that records each user as a sender key. Each of these user's (user1) will have all of the other users (user2) as receiving keys, and the values recorded will be the timestamp of every message sent from user1 to user2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_chat_timestamps():\n",
    "    chat_logs = time_parser('logs/chat_logs.json')\n",
    "    jabber_logs = time_parser('logs/jabber_logs.json')\n",
    "    messages = {}\n",
    "    with open('user_lists/users.txt') as f:\n",
    "        users = f.read().splitlines()\n",
    "    for i in users:\n",
    "        messages[i] = {}\n",
    "    for i in chat_logs:\n",
    "        sender = i['from']\n",
    "        receiver = i['to']\n",
    "        if receiver in messages[sender].keys():\n",
    "            messages[sender][receiver].append(i['ts'])\n",
    "        else: \n",
    "            messages[sender][receiver] = [i['ts']]\n",
    "    for i in jabber_logs:\n",
    "        sender = i['from']\n",
    "        receiver = i['to']\n",
    "        if receiver in messages[sender].keys():\n",
    "            messages[sender][receiver].append(i['ts'])\n",
    "        else: \n",
    "            messages[sender][receiver] = [i['ts']]\n",
    "    for i in messages.keys():\n",
    "        for j in messages[i].keys():\n",
    "            messages[i][j] = sorted(messages[i][j])\n",
    "    return messages\n",
    "interactions = all_chat_timestamps()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will add edges based on meaningful conversations. First, we check to see if both users interact with each other. If they do, we store all of the timestamps of their interactions and use this to extract conversations. A \"conversation\" in our context is messages exchanged where both users are engaging (sending messages) and doing so within a certain time window. We use the number of conversations between two users to establish the weight of the edge between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conversation(interaction_dict):\n",
    "    interactions = []\n",
    "    users = []\n",
    "    for i in interaction_dict.keys():\n",
    "        interactions += interaction_dict[i]\n",
    "        users.append(i)\n",
    "    if len(users) == 1:\n",
    "        user1 = users[0]\n",
    "        user2 = users[0]\n",
    "    else:\n",
    "        user1 = users[0]\n",
    "        user2 = users[1]\n",
    "    interactions.sort()\n",
    "    conversations = []\n",
    "    init_interaction = interactions[0]\n",
    "    if init_interaction in interaction_dict[user1]:\n",
    "        sender2 = user2\n",
    "    else:\n",
    "        sender2 = user1\n",
    "    check_users = False\n",
    "    if user1 == user2: check_users = True\n",
    "    for i in range(len(interactions)-1):\n",
    "        delta_new = interactions[i+1]-interactions[i]\n",
    "        if interactions[i+1] in interaction_dict[sender2]:\n",
    "            check_users = True\n",
    "        if delta_new > datetime.timedelta(hours=8) and check_users:\n",
    "            end_interaction = interactions[i+1]\n",
    "            conversations.append([init_interaction, end_interaction])\n",
    "            if i == len(interactions)-2:\n",
    "                break\n",
    "            init_interaction = interactions[i+2]\n",
    "    return conversations\n",
    "\n",
    "def generate_edges(interactions):\n",
    "    for user1 in interactions.keys():\n",
    "        for user2 in interactions[user1].keys():\n",
    "            inter1 = interactions[user1][user2]\n",
    "            if user2 in interactions.keys():\n",
    "                if user1 in interactions[user2].keys():\n",
    "                    inter2 = interactions[user2][user1]\n",
    "                else:\n",
    "                    inter2 = []\n",
    "            else: inter2 = []\n",
    "            both = {user1:inter1, user2:inter2}\n",
    "            wgt = len(extract_conversation(both))\n",
    "            if wgt > 0:\n",
    "                G.add_edge(user1, user2, key='edge', weight = wgt)\n",
    "generate_edges(interactions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple normalization function, to superimpose a series of values over [0, 1]. We will use this to compute social score to ensure equal weighting of each feature we have selected to use for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dictionary):\n",
    "    n = min(dictionary.values())\n",
    "    m = max(dictionary.values())\n",
    "    for i in dictionary.keys():\n",
    "        dictionary[i] = (dictionary[i]-n) / (m-n)\n",
    "    return dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the values for each user's attributes. Each attribute has its own dictionary, where the users are the keys and the determined value is stored (after normalization). All attributes are calculated using the networkx library of functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/networkx/algorithms/link_analysis/hits_alg.py:78: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A = nx.adjacency_matrix(G, nodelist=list(G), dtype=float)\n"
     ]
    }
   ],
   "source": [
    "user_cliques = {}\n",
    "degrees = G.degree()\n",
    "degrees_dict = {}\n",
    "for i in degrees:\n",
    "    degrees_dict[i[0]] = i[1]\n",
    "degrees_dict = normalize(degrees_dict)\n",
    "betweeness_centrality_dict = normalize(nx.betweenness_centrality(G))\n",
    "degree_centrality_dict = normalize(nx.degree_centrality(G))\n",
    "hubs_authorities = normalize(nx.hits(G)[0])\n",
    "user_clustering_coefficient = {}\n",
    "user_shortest_path = {}\n",
    "for node in Users:\n",
    "    user_cliques[node] = len(nx.cliques_containing_node(G, node))\n",
    "    cur_clustering_coefficient = nx.clustering(G, nodes = node)\n",
    "    user_clustering_coefficient[node] = cur_clustering_coefficient\n",
    "    shortest_path = nx.shortest_path_length(G, source=node).values()\n",
    "    shortest_path_val = sum(shortest_path) / (len(shortest_path))\n",
    "    user_shortest_path[node] = shortest_path_val\n",
    "user_clustering_coefficient = normalize(user_clustering_coefficient)\n",
    "user_shortest_path = normalize(user_shortest_path)\n",
    "user_cliques = normalize(user_cliques)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can calculate the social score. To do so, we iterate through each user and add together all of the associated values from each attribute dictionary. Then, we find the average (since the attributes are currently equally weighted) and store the final score in a dictionary with the user as the key. After we have found each user's score, we sort the dictionary based off social score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_score = {}\n",
    "for u in Users:\n",
    "    social_score[u] = ((degrees_dict[u] + betweeness_centrality_dict[u] + degree_centrality_dict[u] + hubs_authorities[u] + user_clustering_coefficient[u] + user_shortest_path[u] + user_cliques[u])/7)*100\n",
    "social_score = dict(sorted(social_score.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a Pandas dataframe to store our social score values in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(social_score, orient='index', columns = ['Social Score'])\n",
    "df[\"Degree\"] = pd.Series(degrees_dict)\n",
    "df[\"Betweeness Centrality\"] = pd.Series(betweeness_centrality_dict)\n",
    "df[\"Degree Centrality\"] = pd.Series(degree_centrality_dict)\n",
    "df[\"Hubs/Authorities Score\"] = pd.Series(hubs_authorities)\n",
    "df[\"Clustering Coefficient\"] = pd.Series( user_clustering_coefficient)\n",
    "df[\"Average Shortest Path\"] = pd.Series(user_shortest_path)\n",
    "df[\"Number of Cliques\"] = pd.Series(user_cliques)\n",
    "#display(df[399:])\n",
    "pd.DataFrame.to_csv(df, path_or_buf=\"score.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also will store our final graph in a .gexf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G, 'conti_meaningful.gexf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
