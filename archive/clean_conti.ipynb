{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will take a directory of chat logs and merge them. We will take our desired directory path and output file name and return a single json file with all logs.\n",
    "\n",
    "Note that we run file formatting on each singular json file to add the proper delimiters and formatting, then run it again on our output file to ensure our formatting is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os\n",
    "import argparse\n",
    "import dateutil.parser\n",
    "import numpy\n",
    "\n",
    "#Formats file into JSON file\n",
    "def file_formatting(fp):\n",
    "    parser = json.JSONDecoder()\n",
    "    parsed = []  # a list to hold individually parsed JSON structures\n",
    "    with open(fp) as f:\n",
    "        data = f.read()\n",
    "    head = 0  # hold the current position as we parse\n",
    "    while True:\n",
    "        head = (data.find('{', head) + 1 or data.find('[', head) + 1) - 1\n",
    "        try:\n",
    "            struct, head = parser.raw_decode(data, head)\n",
    "            parsed.append(struct)\n",
    "        except (ValueError, json.JSONDecodeError):  # no more valid JSON structures\n",
    "            break\n",
    "    json_obj = json.dumps(parsed, indent=2)\n",
    "    return json_obj\n",
    "\n",
    "#Merges a directory of JSON files into one\n",
    "def merge_chats(dir, output):\n",
    "    result = []\n",
    "    with open(output, \"w\") as out:\n",
    "        for file in os.listdir(dir):\n",
    "            #print(file)\n",
    "            #input()\n",
    "            f = os.path.join(dir, file)\n",
    "            if os.path.isfile(f):\n",
    "                out.write(file_formatting(f))\n",
    "    result = file_formatting(output)\n",
    "    with open(output, \"w\") as out:\n",
    "        out.write(result)\n",
    "\n",
    "#Merge all chats into one json file\n",
    "chat_dir = \"Conti Chat Logs 2020\"\n",
    "chat_out = \"logs/chat_logs.json\"\n",
    "merge_chats(chat_dir,chat_out)\n",
    "\n",
    "#Merge all jabber into one json file\n",
    "jabber_dir = \"Conti Jabber Chat Logs 2021 - 2022\"\n",
    "jabber_out = \"logs/jabber_logs.json\"\n",
    "merge_chats(jabber_dir,jabber_out )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will find a unique list of users from a JSON file we input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a unique np array of users from a JSON file\n",
    "def user_parser(data_json):\n",
    "    users = []\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-i', action='store')\n",
    "    filename = data_json\n",
    "    with open(filename) as f:\n",
    "        json_data = json.load(f)\n",
    "    for i in (json_data):\n",
    "        user_str = i['from'].lower()\n",
    "        lst = user_str.split('@')\n",
    "        users.append(lst[0]) \n",
    "        i['from'] = lst[0]\n",
    "        user_str = i['to'].lower()\n",
    "        lst = user_str.split('@')\n",
    "        users.append(lst[0]) \n",
    "        i['to'] = lst[0]\n",
    "    users_np = numpy.array(users)\n",
    "    return numpy.unique(users_np), json_data\n",
    "\n",
    "#Updates master files with only username\n",
    "def clean_users(data_json, output):\n",
    "    with open(output, 'w') as out:\n",
    "        out.write(json.dumps(data_json))\n",
    "\n",
    "jabber_users, jab_json_with_usernames = user_parser(jabber_out)\n",
    "clean_users(jab_json_with_usernames, jabber_out)\n",
    "chat_users, chat_json_with_usernames = user_parser(chat_out)\n",
    "clean_users(chat_json_with_usernames, chat_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our time parser accepts a json file and will return a json object with the timestamps converted to datetime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns JSON object with datetime value \n",
    "def time_parser(data_json):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-i', action='store')\n",
    "    filename = data_json\n",
    "    with open(filename) as f:\n",
    "        json_data = json.load(f)\n",
    "    for i in (json_data):\n",
    "        i['ts'] = dateutil.parser.isoparse(i['ts']) # ISO 8601 extended format\n",
    "    return json_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic analysis/ numbers about our users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in both: 183\n",
      "['admin' 'admintest' 'admu' 'ahtyng' 'alarm' 'alarm2' 'alaska' 'ali'\n",
      " 'alter' 'andy' 'atlant' 'axel' 'azot' 'baget' 'baly' 'band' 'batka'\n",
      " 'baxter' 'bekeeper' 'bentley' 'bill' 'bob' 'boby' 'bonen' 'bourbon' 'bra'\n",
      " 'braun' 'brom' 'buggati' 'bullet' 'bumer' 'buza' 'calmar' 'cany' 'carter'\n",
      " 'casper' 'ceram' 'chaos' 'chip' 'clipper' 'contisupport' 'cooler'\n",
      " 'cosmos' 'cruz' 'dandis' 'dandmen' 'darc' 'defender' 'deploy' 'derek'\n",
      " 'dick' 'dominik' 'doomsday' 'dove' 'driver' 'electronic' 'elon' 'fast'\n",
      " 'flip' 'food' 'forbes' 'ford' 'forest' 'fox' 'frank' 'frog' 'ganesh'\n",
      " 'germes' 'ghost' 'globus' 'grajdanin' 'grant' 'green' 'gringo' 'grom'\n",
      " 'gus' 'hash' 'hlor' 'hof' 'idgo' 'jumbo' 'kagas' 'kaktus' 'kent'\n",
      " 'kerasid' 'kevin' 'killer' 'kingston' 'kramer' 'kran' 'lemur' 'leo'\n",
      " 'licor' 'log' 'logan' 'longer' 'love' 'lucas' 'mango' 'many' 'marcus'\n",
      " 'marsel' 'matiz' 'mavelek' 'mavemat' 'max' 'mentos' 'merch' 'miguel'\n",
      " 'modar' 'modnik' 'moms' 'moon' 'morgan' 'mors' 'muchacho' 'mushroom'\n",
      " 'naned' 'neo' 'nevada' 'nik-da' 'oldtimes' 'oliver' 'olsen' 'paranoik'\n",
      " 'pin' 'pincus' 'pineapple' 'poll' 'ponetre' 'price' 'professor' 'ramon'\n",
      " 'rand' 'redmond' 'reshaev' 'revan' 'revers' 'rozetka' 'salamandra' 'sand'\n",
      " 'sharn' 'shell' 'skywalker' 'snow' 'sonar' 'spider' 'spoon' 'staff'\n",
      " 'stakan' 'steller' 'stern' 'steve' 'strix' 'summit' 'sunday' 'swift'\n",
      " 'taker' 'target' 'tatarin' 'taur' 'terry' 'test' 'tiktak' 'tilar' 'tom'\n",
      " 'toris' 'troy' 'trumen' 'tunri' 'twin' 'urbanone' 'vampire' 'van' 'vertu'\n",
      " 'viper' 'werka' 'winston' 'xenon' 'xoc' 'zevs' 'zloysobaka' 'zulas']\n",
      "Number of users only in Jabber: 154\n",
      "['air' 'alto' 'answer' 'atlas' 'b2zbrzj5sljbmemgfqnn7zlarduyjhvjwt'\n",
      " 'badboy' 'baraka' 'baron' 'bash' 'begemot' 'beny' 'best' 'beta' 'bezdar'\n",
      " 'billgeizh' 'bio' 'blackjob' 'blood' 'bloodrush' 'born'\n",
      " 'btdpnqwg2nqkqceetqnn7zlarduyjhvjwt' 'buran' 'caution' 'cert' 'chain'\n",
      " 'cheesecake' 'cherry' 'cicada' 'cnn' 'creamsod' 'cybergangster' 'dantis'\n",
      " 'def' 'demetrius' 'demon' 'derekson' 'dino' 'doctor' 'dollar' 'duna'\n",
      " 'dylan' 'dylon' 'ed' 'eleos' 'eleoslin' 'elvira' 'exploit.im' 'fatboy'\n",
      " 'fire' 'firefox333' 'flint' 'fly' 'fog' 'forum' 'freebeer' 'front'\n",
      " 'frost' 'fury' 'git' 'glad' 'gm' 'goga' 'gold' 'golova' 'goodwin' 'gorec'\n",
      " 'graf' 'grafin' 'gram' 'grand' 'grover' 'guava' 'gucci' 'hitech' 'hod'\n",
      " 'hookahplace' 'hors' 'horse' 'impact' 'inkognito' 'jax' 'johnyboy77'\n",
      " 'joynses' 'kintaro' 'klaus' 'kolbasa' 'kolin' 'larry' 'loft' 'lom'\n",
      " 'macallan' 'mashroom' 'master' 'mavelak' 'max17' 'meatball' 'miner'\n",
      " 'mitzi' 'mont' 'mops' 'morisson' 'mozart' 'mult' 'nek' 'netman'\n",
      " 'netwalker' 'nick' 'nuggets'\n",
      " 'odw5mdwotufuxxrgw3pvqjjuze3e33bylylkl667h4nefwiimwqsumyd.onion'\n",
      " 'packman' 'page' 'painkiller' 'pin2' 'porovoz' 'prizrak' 'pumba'\n",
      " 'q3mcco35auwcstmt.onion' 'qwerqwerqwerqwer' 'qwerty' 'romanov'\n",
      " 'romanov_2' 'rooty' 'salmon21' 'santi' 'sentinel' 'sepvilk' 'serp'\n",
      " 'seven300' 'shamm' 'shark' 'skippy' 'slojno' 'soul' 'specter' 'star'\n",
      " 'starfall' 'sticks' 'stigg' 'subzero' 'taobao' 'tibone' 'tiniles' 'tort'\n",
      " 'tramp' 'ttrr' 'valemy' 'verchunls' 'veron' 'void' 'wertu' 'wind'\n",
      " 'wowddoz' 'xenkee' 'zolotoy']\n",
      "Number of users only in Chat: 112\n",
      "['0x00lord' '8383' 'ahtung' 'airbnb1' 'alert' 'aloxa' 'balzak' 'barmen'\n",
      " 'bestofthebest' 'black' 'boba' 'booker' 'buh' 'buri' 'cameron' 'cesar'\n",
      " 'child' 'chrom' 'clickclack' 'cobdoctor' 'craft' 'cuba' 'dallas' 'david'\n",
      " 'delta' 'dereksupp' 'doloto' 'domovoy' 'dorirus' 'duke' 'efrain' 'elvis'\n",
      " 'expex' 'fasker' 'fergus' 'fff' 'finn' 'fischer' 'focus' 'forus'\n",
      " 'gentleman' 'gideon777' 'good' 'good_place' 'grossman' 'hopkins' 'host'\n",
      " 'huanivan' 'ilon' 'inat' 'info' 'ivanalert' 'jafar' 'jora' 'kerberos'\n",
      " 'keykey' 'koncord' 'kurt' 'loadsupport1' 'loadsupport2' 'macros' 'mario'\n",
      " 'mark' 'mavalek' 'merlin' 'muhoboi' 'noman' 'ololoenko' 'oscar' 'panda'\n",
      " 'parker' 'perry' 'phantom' 'private' 'proffjeck' 'project_talk' 'quite'\n",
      " 'qwertycatt' 'redroom' 'rox' 'sandy' 'saulgdmn' 'savage' 'sega' 'shaper'\n",
      " 'sirafim' 'slon' 'song' 'stefan' 'support-100' 'talar' 'tnt' 'total'\n",
      " 'trump' 'tunotif' 'twinq3mcco35auwcstmt.onion' 'twister' 'urban' 'v1cev1'\n",
      " 'vang' 'victor' 'vjud.q3mcco35auwcstmt.onion' 'voron' 'watson' 'weav'\n",
      " 'workman1' 'workman2' 'xargs' 'xmoney' 'xnull' 'xxx' 'стов']\n"
     ]
    }
   ],
   "source": [
    "#Evaluates user lists\n",
    "both = [i for i in jabber_users if i in chat_users]\n",
    "unique_chat = [i for i in chat_users if i not in jabber_users]\n",
    "unique_jabber = [i for i in jabber_users if i not in chat_users]\n",
    "all_users = numpy.union1d(jabber_users, chat_users)\n",
    "with open('user_lists/users.txt', 'w') as file:\n",
    "    numpy.savetxt(file, all_users, fmt='%s')\n",
    "with open('user_lists/jabber_unique_users.txt', 'w') as file:\n",
    "    numpy.savetxt(file, unique_jabber, fmt='%s')\n",
    "with open('user_lists/chat_unique_users.txt', 'w') as file:\n",
    "    numpy.savetxt(file, unique_chat, fmt='%s')\n",
    "\n",
    "print(\"Number of users in both:\", len(both))\n",
    "print(numpy.array(both))\n",
    "print(\"Number of users only in Jabber:\", len(unique_jabber))\n",
    "print(numpy.array(unique_jabber))\n",
    "print(\"Number of users only in Chat:\", len(unique_chat))\n",
    "print(numpy.array(unique_chat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
